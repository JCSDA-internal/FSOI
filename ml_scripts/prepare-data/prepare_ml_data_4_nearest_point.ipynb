{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><font color = 'blue'>Prepare ML data 4 nearest pts</font></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import boto3\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "from itertools import product\n",
    "from math import trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.4.10\n",
      "  latest version: 4.5.11\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs: \n",
      "    - netcdf4\n",
      "    - xarray\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2018.8.24          |           py36_1         140 KB\n",
      "    libgcc-ng-8.2.0            |       hdf63c60_1         7.6 MB\n",
      "    numpy-1.14.3               |   py36h28100ab_1          41 KB\n",
      "    xarray-0.10.8              |           py36_0         730 KB\n",
      "    netcdf4-1.4.1              |   py36h4b4f87f_0         532 KB\n",
      "    hdf4-4.2.13                |       h3ca952b_2         916 KB\n",
      "    cftime-1.0.0b1             |   py36h035aef0_0         260 KB\n",
      "    libnetcdf-4.6.1            |       h13459d8_0         1.2 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        11.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    cftime:          1.0.0b1-py36h035aef0_0                           \n",
      "    hdf4:            4.2.13-h3ca952b_2                                \n",
      "    libnetcdf:       4.6.1-h13459d8_0                                 \n",
      "    netcdf4:         1.4.1-py36h4b4f87f_0                             \n",
      "    xarray:          0.10.8-py36_0                                    \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    certifi:         2018.8.13-py36_0                      conda-forge --> 2018.8.24-py36_1     \n",
      "    libgcc-ng:       7.2.0-hdf63c60_3                                  --> 8.2.0-hdf63c60_1     \n",
      "    openssl:         1.0.2o-h470a237_1                     conda-forge --> 1.0.2p-h14c3975_0    \n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "    blas:            1.1-openblas                          conda-forge --> 1.0-mkl              \n",
      "    ca-certificates: 2018.4.16-0                           conda-forge --> 2018.03.07-0         \n",
      "    numpy:           1.15.0-py36_blas_openblashd3ea46f_200 conda-forge [blas_openblas] --> 1.14.3-py36h28100ab_1\n",
      "    opencv:          3.4.1-py36_blas_openblash829a850_201  conda-forge [blas_openblas] --> 3.4.1-py36h6fd60c2_1 \n",
      "    scikit-learn:    0.19.2-py36_blas_openblasha84fab4_201 conda-forge [blas_openblas] --> 0.19.1-py36h7aa7ec6_0\n",
      "    scipy:           1.1.0-py36_blas_openblash7943236_201  conda-forge [blas_openblas] --> 1.1.0-py36hfc37229_0 \n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi 2018.8.24: ##################################################### | 100% \n",
      "libgcc-ng 8.2.0: ####################################################### | 100% \n",
      "numpy 1.14.3: ########################################################## | 100% \n",
      "xarray 0.10.8: ######################################################### | 100% \n",
      "netcdf4 1.4.1: ######################################################### | 100% \n",
      "hdf4 4.2.13: ########################################################### | 100% \n",
      "cftime 1.0.0b1: ######################################################## | 100% \n",
      "libnetcdf 4.6.1: ####################################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y xarray netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import boto3\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "from itertools import product\n",
    "from math import trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that import level names from a txt file\n",
    "def get_levels(resource):\n",
    "    filename = \"levels.txt\"\n",
    "    resource.Bucket(\"fsoi\").download_file(filename, filename)\n",
    "\n",
    "    f = open(filename, \"r\")\n",
    "    levels = f.read().split(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    os.remove(filename)\n",
    "\n",
    "    return levels\n",
    "\n",
    "\n",
    "# define a function that creates lat, lon dicts which for an obs lat or lon\n",
    "# return the two nearest in bgk\n",
    "def get_coord_dicts():\n",
    "    # get lat and lon bkg possible values\n",
    "    lat_bkg = np.arange(-90, 90.5, 0.5)\n",
    "    lon_bkg = np.arange(-180, 180.0, 0.625) # no 180 lon value in bkg\n",
    "    \n",
    "    # get lat and lon obs possible values\n",
    "    lat_keys = np.arange(-90., 90.01, 0.01)\n",
    "    lon_keys = np.arange(-180., 180.01, 0.01)\n",
    "    \n",
    "    # for each possible obs value get the 2 nearest bkg values\n",
    "    lat_values = [sorted(lat_bkg, key=lambda x: abs(x - lat))[:2] for lat in lat_keys]\n",
    "    lon_values = [sorted(lon_bkg, key=lambda x: abs(x - lon))[:2] for lon in lon_keys]\n",
    "    \n",
    "    # join keys and values into dict\n",
    "    lat_dict = dict(zip(np.round(lat_keys, 2), lat_values))\n",
    "    lon_dict = dict(zip(np.round(lon_keys, 2), lon_values))\n",
    "    \n",
    "    return lat_dict, lon_dict\n",
    "\n",
    "\n",
    "# define a function that loads obs and bkg data into pandas dataframes\n",
    "def load_data(filepath, date):\n",
    "    resource = boto3.resource(\"s3\")\n",
    "    filename = \"/tmp/\" + filepath.split(\"/\")[-1]\n",
    "    resource.Bucket(\"fsoi\").download_file(filepath, filename)\n",
    "    \n",
    "    obs = pd.read_hdf(filename).xs([\"AMSUA_N18\", 7], level=[\"PLATFORM\", \"CHANNEL\"])\n",
    "    os.remove(filename)\n",
    "    \n",
    "    obs = obs.reset_index(level=[0, 1])\n",
    "    obs = obs.drop([\"OBTYPE\", \"OBERR\", \"PRESSURE\"], axis=1)\n",
    "    \n",
    "    # fix lon between -180/180 instead of 0/360\n",
    "    mask_lon = obs[obs[\"LONGITUDE\"] > 180].index.tolist()\n",
    "    obs.loc[mask_lon, \"LONGITUDE\"] = obs.loc[mask_lon, \"LONGITUDE\"] - 360\n",
    "    \n",
    "    date = date[:-2] + \"_\" + date[-2:]\n",
    "    month = date[:4] + \"_\" + date[4:6] + \"/\"\n",
    "    filename = \"e5130_hyb_01.bkg.eta.\" + date + \"z.nc4\"\n",
    "    filepath = \"bkg/\" + month + filename\n",
    "    filename = \"/tmp/\" + filename\n",
    "    resource.Bucket(\"fsoi\").download_file(filepath, filename)\n",
    "    \n",
    "    bkg = xr.open_dataset(filename).to_dataframe()\\\n",
    "            .reset_index(level=[0, 1, 2, 3]).drop('time', axis=1)\n",
    "    \n",
    "    os.remove(filename)\n",
    "            \n",
    "    obs = obs.sort_values([\"LONGITUDE\", \"LATITUDE\"]).reset_index(drop=True)\n",
    "    bkg = bkg.sort_values([\"lon\", \"lat\"]).reset_index(drop=True)\n",
    "    \n",
    "    # fix bkg zeros lat and lon to 0.0 instead of something like -1.79751e-13\n",
    "    zero_lat_mask = bkg[(bkg['lat'] < 0.1) & (bkg['lat'] > -0.1)].index.tolist()\n",
    "    zero_lon_mask = bkg[(bkg['lon'] < 0.1) & (bkg['lon'] > -0.1)].index.tolist()\n",
    "    \n",
    "    bkg.loc[zero_lat_mask, 'lat'] = 0.0\n",
    "    bkg.loc[zero_lon_mask, 'lon'] = 0.0\n",
    "    \n",
    "    bkg.set_index(['lat', 'lon'], inplace=True)\n",
    "        \n",
    "    return obs, bkg\n",
    "\n",
    "\n",
    "# define a function that for an obs append a list with the 4 nearest bkg pts\n",
    "def get_nearest_pts(obs_row, nearest_pts, lat_dict, lon_dict):\n",
    "    lat = np.round(obs_row.LATITUDE, 2)\n",
    "    lon = np.round(obs_row.LONGITUDE, 2)\n",
    "    \n",
    "    row_nearest_pts = [\n",
    "            (lat_dict[lat][i], lon_dict[lon][j])\n",
    "            for i, j in product([0, 1], [0, 1])\n",
    "            ]\n",
    "        \n",
    "    nearest_pts.append(row_nearest_pts)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# define a function that gets bkg levels data for a particular point\n",
    "def get_lev_data(bkg, ix, level_cols):\n",
    "    return bkg.loc[ix:ix + 71, level_cols[1:]].copy().stack().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# define a function that adds level data from bkg 3D to 2D by transposing it\n",
    "def add_lev_data(bkg_3D, bkg_2D, obs, level_cols, levels, i):\n",
    "    lev_data = []\n",
    "    \n",
    "    bkg_2D.index.map(\n",
    "            lambda ix: lev_data.append(get_lev_data(bkg_3D, ix, level_cols))\n",
    "            )\n",
    "    \n",
    "    lev_data = pd.concat(lev_data, axis=1).transpose().reset_index(drop=True)\n",
    "        \n",
    "    lev_data.columns = [\n",
    "            col + '_' + lev\n",
    "            for lev in levels\n",
    "            for col in level_cols[1:]\n",
    "            ]\n",
    "    \n",
    "    bkg_2D = pd.concat([bkg_2D.reset_index(drop=True), lev_data], axis=1)\n",
    "    bkg_2D = bkg_2D.add_prefix('point' + str(i + 1) + '_')\n",
    "    \n",
    "    return bkg_2D\n",
    "\n",
    "\n",
    "# define a function that get bkg nearest data from observations\n",
    "def get_nearest_bkg(obs, bkg, lat_dict, lon_dict, level_cols, levels):\n",
    "    nearest_pts, nearest_bkg = [], []\n",
    "    \n",
    "    obs.apply(lambda row: get_nearest_pts(row, nearest_pts, lat_dict,\n",
    "                                          lon_dict), axis=1)\n",
    "    \n",
    "    pts = [\n",
    "            [points[i] for points in nearest_pts]\n",
    "            for i in range(0, len(nearest_pts[0]))\n",
    "            ]\n",
    "    \n",
    "    for i, pts_i in enumerate(pts):\n",
    "        bkg_3D_i = bkg.loc[pts_i].reset_index(level=[0, 1])\n",
    "        \n",
    "        mask_2D = np.arange(0, len(bkg_3D_i), 72)\n",
    "        bkg_2D_i = bkg_3D_i.drop(level_cols, axis=1).loc[mask_2D, :]\n",
    "        bkg_2D_i = add_lev_data(bkg_3D_i, bkg_2D_i, obs, level_cols, levels, i)\n",
    "        \n",
    "        nearest_bkg.append(bkg_2D_i)\n",
    "        \n",
    "    nearest_bkg = pd.concat(nearest_bkg, axis=1)\n",
    "    \n",
    "    return nearest_bkg\n",
    "\n",
    "\n",
    "# define a function that merges our training data from all files\n",
    "def merge_train_data(filepaths, i, n, lat_dict, lon_dict, levels):\n",
    "    level_cols = [\"lev\", \"delp\", \"u\", \"v\", \"tv\", \"sphu\", \"ozone\", \"qitot\", \"qltot\"]\n",
    "    ml_data = pd.DataFrame()\n",
    "    \n",
    "    for j in range(i, len(filepaths), n):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        filepath = filepaths[j]\n",
    "        date = re.findall(\"[0-9]+.h5\", filepath)[0][:-3]\n",
    "        obs, bkg = load_data(filepath, date)\n",
    "        \n",
    "        nearest_bkg = get_nearest_bkg(obs, bkg, lat_dict, lon_dict, level_cols,\n",
    "                                      levels)\n",
    "        \n",
    "        merge = pd.concat([obs, nearest_bkg], axis=1)\n",
    "        ml_data = pd.concat([ml_data, merge], axis=0)\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Merge {} obs and bkg done in: {} min and {} sec\".format(\n",
    "            date, trunc((end - start)/60), round((end - start)%60)\n",
    "        ))\n",
    "        \n",
    "    return ml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 2015020206 obs and bkg done in: 1 min and 58 sec\n",
      "Merge 2015020212 obs and bkg done in: 1 min and 58 sec\n",
      "Merge 2015020118 obs and bkg done in: 2 min and 1 sec\n",
      "Merge 2015020112 obs and bkg done in: 2 min and 6 sec\n",
      "Merge 2015020406 obs and bkg done in: 2 min and 3 sec\n",
      "Merge 2015020100 obs and bkg done in: 2 min and 10 sec\n",
      "Merge 2015020106 obs and bkg done in: 2 min and 22 sec\n",
      "Merge 2015020300 obs and bkg done in: 3 min and 13 sec\n",
      "Merge 2015020200 obs and bkg done in: 3 min and 19 sec\n",
      "Merge 2015020312 obs and bkg done in: 3 min and 26 sec\n",
      "Merge 2015020400 obs and bkg done in: 3 min and 55 sec\n",
      "Merge 2015020700 obs and bkg done in: 2 min and 7 sec\n",
      "Merge 2015020512 obs and bkg done in: 2 min and 1 sec\n",
      "Merge 2015020818 obs and bkg done in: 2 min and 3 sec\n",
      "Merge 2015020506 obs and bkg done in: 4 min and 9 sec\n",
      "Merge 2015020618 obs and bkg done in: 2 min and 19 sec\n",
      "Merge 2015020518 obs and bkg done in: 2 min and 2 sec\n",
      "Merge 2015020218 obs and bkg done in: 4 min and 22 sec\n",
      "Merge 2015020418 obs and bkg done in: 4 min and 19 sec\n",
      "Merge 2015020318 obs and bkg done in: 4 min and 22 sec\n",
      "Merge 2015020500 obs and bkg done in: 4 min and 20 sec\n",
      "Merge 2015020600 obs and bkg done in: 2 min and 22 sec\n",
      "Merge 2015020412 obs and bkg done in: 4 min and 26 sec\n",
      "Merge 2015020306 obs and bkg done in: 4 min and 30 sec\n",
      "Merge 2015020606 obs and bkg done in: 2 min and 58 sec\n",
      "Merge 2015020612 obs and bkg done in: 2 min and 5 sec\n",
      "Merge 2015020712 obs and bkg done in: 2 min and 11 sec\n",
      "Merge 2015020800 obs and bkg done in: 2 min and 9 sec\n",
      "Merge 2015020812 obs and bkg done in: 2 min and 6 sec\n",
      "Merge 2015021000 obs and bkg done in: 2 min and 8 sec\n",
      "Merge 2015021112 obs and bkg done in: 2 min and 11 sec\n",
      "Merge 2015021306 obs and bkg done in: 2 min and 10 sec\n",
      "Merge 2015020918 obs and bkg done in: 2 min and 9 sec\n",
      "Merge 2015021006 obs and bkg done in: 2 min and 6 sec\n",
      "Merge 2015020706 obs and bkg done in: 2 min and 5 sec\n",
      "Merge 2015021106 obs and bkg done in: 2 min and 12 sec\n",
      "Merge 2015021012 obs and bkg done in: 2 min and 7 sec\n",
      "Merge 2015020906 obs and bkg done in: 2 min and 11 sec\n",
      "Merge 2015020900 obs and bkg done in: 2 min and 6 sec\n",
      "Merge 2015020718 obs and bkg done in: 2 min and 5 sec\n",
      "Merge 2015020806 obs and bkg done in: 2 min and 13 sec\n",
      "Merge 2015020912 obs and bkg done in: 2 min and 31 sec\n",
      "Merge 2015021018 obs and bkg done in: 2 min and 8 sec\n",
      "Merge 2015021100 obs and bkg done in: 2 min and 6 sec\n",
      "Merge 2015021200 obs and bkg done in: 2 min and 9 sec\n",
      "Merge 2015021212 obs and bkg done in: 2 min and 3 sec\n",
      "Merge 2015021300 obs and bkg done in: 2 min and 7 sec\n",
      "Merge 2015021718 obs and bkg done in: 1 min and 56 sec\n",
      "Merge 2015021412 obs and bkg done in: 2 min and 9 sec\n",
      "Merge 2015021600 obs and bkg done in: 2 min and 10 sec\n",
      "Merge 2015021312 obs and bkg done in: 1 min and 56 sec\n",
      "Merge 2015021418 obs and bkg done in: 2 min and 5 sec\n",
      "Merge 2015021406 obs and bkg done in: 2 min and 12 sec\n",
      "Merge 2015021518 obs and bkg done in: 2 min and 6 sec\n",
      "Merge 2015021318 obs and bkg done in: 2 min and 1 sec\n",
      "Merge 2015021118 obs and bkg done in: 2 min and 14 sec\n",
      "Merge 2015021206 obs and bkg done in: 2 min and 6 sec\n",
      "Merge 2015021218 obs and bkg done in: 2 min and 7 sec\n",
      "Merge 2015021500 obs and bkg done in: 2 min and 25 sec\n",
      "Merge 2015021400 obs and bkg done in: 2 min and 9 sec\n",
      "Merge 2015021506 obs and bkg done in: 2 min and 4 sec\n",
      "Merge 2015021512 obs and bkg done in: 2 min and 9 sec\n",
      "Merge 2015021612 obs and bkg done in: 2 min and 8 sec\n",
      "Merge 2015021700 obs and bkg done in: 2 min and 7 sec\n",
      "Merge 2015021712 obs and bkg done in: 2 min and 5 sec\n",
      "Merge 2015022206 obs and bkg done in: 2 min and 11 sec\n",
      "Merge 2015021900 obs and bkg done in: 2 min and 8 sec\n",
      "Merge 2015021806 obs and bkg done in: 2 min and 3 sec\n",
      "Merge 2015021706 obs and bkg done in: 1 min and 55 sec\n",
      "Merge 2015022012 obs and bkg done in: 2 min and 14 sec\n",
      "Merge 2015021818 obs and bkg done in: 2 min and 6 sec\n",
      "Merge 2015021906 obs and bkg done in: 2 min and 11 sec\n",
      "Merge 2015022006 obs and bkg done in: 2 min and 9 sec\n",
      "Merge 2015021800 obs and bkg done in: 2 min and 13 sec\n",
      "Merge 2015021618 obs and bkg done in: 2 min and 7 sec\n",
      "Merge 2015021606 obs and bkg done in: 2 min and 12 sec\n",
      "Merge 2015021912 obs and bkg done in: 2 min and 8 sec\n",
      "Merge 2015021812 obs and bkg done in: 2 min and 9 sec\n",
      "Merge 2015021918 obs and bkg done in: 2 min and 8 sec\n",
      "Merge 2015022000 obs and bkg done in: 2 min and 9 sec\n",
      "Merge 2015022100 obs and bkg done in: 2 min and 8 sec\n",
      "Merge 2015022112 obs and bkg done in: 2 min and 8 sec\n",
      "Merge 2015022200 obs and bkg done in: 2 min and 7 sec\n",
      "Merge 2015022618 obs and bkg done in: 2 min and 1 sec\n",
      "Merge 2015022312 obs and bkg done in: 2 min and 8 sec\n",
      "Merge 2015022218 obs and bkg done in: 2 min and 4 sec\n",
      "Merge 2015022118 obs and bkg done in: 2 min and 7 sec\n",
      "Merge 2015022306 obs and bkg done in: 2 min and 7 sec\n",
      "Merge 2015022318 obs and bkg done in: 2 min and 5 sec\n",
      "Merge 2015022500 obs and bkg done in: 2 min and 13 sec\n",
      "Merge 2015022212 obs and bkg done in: 2 min and 12 sec\n",
      "Merge 2015022106 obs and bkg done in: 2 min and 9 sec\n",
      "Merge 2015022018 obs and bkg done in: 2 min and 6 sec\n",
      "Merge 2015022418 obs and bkg done in: 2 min and 19 sec\n",
      "Merge 2015022400 obs and bkg done in: 2 min and 9 sec\n",
      "Merge 2015022406 obs and bkg done in: 2 min and 0 sec\n",
      "Merge 2015022300 obs and bkg done in: 2 min and 7 sec\n",
      "Merge 2015022412 obs and bkg done in: 2 min and 6 sec\n",
      "Merge 2015022512 obs and bkg done in: 2 min and 6 sec\n",
      "Merge 2015022600 obs and bkg done in: 2 min and 8 sec\n",
      "Merge 2015022612 obs and bkg done in: 2 min and 9 sec\n",
      "Merge 2015022800 obs and bkg done in: 2 min and 3 sec\n",
      "Merge 2015022706 obs and bkg done in: 2 min and 7 sec\n",
      "Merge 2015022606 obs and bkg done in: 2 min and 5 sec\n",
      "Merge 2015022718 obs and bkg done in: 2 min and 6 sec\n",
      "Merge 2015022806 obs and bkg done in: 2 min and 5 sec\n",
      "Merge 2015022518 obs and bkg done in: 1 min and 56 sec\n",
      "Merge 2015022700 obs and bkg done in: 2 min and 3 sec\n",
      "Merge 2015022506 obs and bkg done in: 2 min and 5 sec\n",
      "Merge 2015022812 obs and bkg done in: 2 min and 8 sec\n",
      "Merge 2015022712 obs and bkg done in: 2 min and 5 sec\n",
      "Merge 2015022818 obs and bkg done in: 2 min and 7 sec\n",
      "Saved and compressed in: 28 min and 43 sec\n",
      "Total program took: 0 hours and 46 min\n"
     ]
    }
   ],
   "source": [
    "pstart = time.time()\n",
    "\n",
    "client = boto3.client(\"s3\")\n",
    "resource = boto3.resource(\"s3\")\n",
    "\n",
    "lat_dict, lon_dict = get_coord_dicts()\n",
    "levels = get_levels(resource)\n",
    "\n",
    "months = [\"2014_12\", \"2015_01\", \"2015_02\"]\n",
    "filepaths = []\n",
    "\n",
    "for month in months:\n",
    "    monthpaths = client.list_objects(Bucket=\"fsoi\", Prefix=\"obs/GMAO_\" + month + \"/GMAO.dry.\")\n",
    "    filepaths += [dic[\"Key\"] for dic in monthpaths[\"Contents\"]]\n",
    "\n",
    "# define how many process we want to run\n",
    "n = 18\n",
    "pool = mp.Pool(processes=n)\n",
    "\n",
    "# run our processes\n",
    "results = [\n",
    "        pool.apply_async(merge_train_data, args=(filepaths[248:], i, n, lat_dict, # PERFORM MONTH BY MONTH TO AVOID \n",
    "                                                 lon_dict, levels))               # MEMORY ERROR(too large to unpickle)\n",
    "        for i in range(n)\n",
    "        ]\n",
    "\n",
    "results = [p.get() for p in results]\n",
    "\n",
    "# merge our training data\n",
    "ml_data = pd.concat(results, axis=0)\n",
    "\n",
    "ml_data = ml_data.sort_values([\"DATETIME\", \"LATITUDE\", \"LONGITUDE\"])\\\n",
    "                 .reset_index(drop=True)\n",
    "\n",
    "# save and compress training data in hdf5 format\n",
    "start = time.time()\n",
    "                             \n",
    "ml_data.to_hdf(\"../efs/data/amsua_n18_ch7_4pts_part3.h5\", key=\"df\", complevel=9)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Saved and compressed in: {} min and {} sec\".format(trunc((end - start)/60),\n",
    "                                                          round((end - start)%60)))\n",
    "    \n",
    "# display total program time\n",
    "pend = time.time()\n",
    "print(\"Total program took: {} hours and {} min\".format(trunc((pend - pstart)/3600),\n",
    "                                                       round((pend - pstart)%3600/60)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
