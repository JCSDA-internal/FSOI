{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><font color = 'blue'>Prepare ML data</font></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import boto3\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from math import trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.4.10\n",
      "  latest version: 4.5.9\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs: \n",
      "    - netcdf4\n",
      "    - xarray\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    libnetcdf-4.6.1            |       h13459d8_0         1.2 MB\n",
      "    xarray-0.10.8              |           py36_0         730 KB\n",
      "    pytables-3.4.4             |   py36ha205bf6_0         1.5 MB\n",
      "    h5py-2.8.0                 |   py36h8d01980_0         1.1 MB\n",
      "    openssl-1.0.2o             |       h14c3975_1         3.4 MB\n",
      "    netcdf4-1.4.0              |   py36ha06eab4_1         536 KB\n",
      "    hdf4-4.2.13                |       h3ca952b_2         916 KB\n",
      "    cftime-1.0.0b1             |   py36h035aef0_0         260 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         9.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    cftime:          1.0.0b1-py36h035aef0_0                           \n",
      "    hdf4:            4.2.13-h3ca952b_2                                \n",
      "    libnetcdf:       4.6.1-h13459d8_0                                 \n",
      "    netcdf4:         1.4.0-py36ha06eab4_1                             \n",
      "    xarray:          0.10.8-py36_0                                    \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    certifi:         2018.4.16-py36_0                      conda-forge --> 2018.4.16-py36_0     \n",
      "    h5py:            2.8.0-py36h470a237_0                  conda-forge --> 2.8.0-py36h8d01980_0 \n",
      "    hdf5:            1.10.1-2                              conda-forge --> 1.10.2-hba1933b_1    \n",
      "    openssl:         1.0.2o-0                              conda-forge --> 1.0.2o-h14c3975_1    \n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "    blas:            1.1-openblas                          conda-forge --> 1.0-mkl              \n",
      "    ca-certificates: 2018.4.16-0                           conda-forge --> 2018.03.07-0         \n",
      "    numpy:           1.14.5-py36_blas_openblashd3ea46f_200 conda-forge [blas_openblas] --> 1.14.3-py36h28100ab_2\n",
      "    opencv:          3.4.1-py36_blas_openblas_200          conda-forge [blas_openblas] --> 3.4.1-py36h6fd60c2_1 \n",
      "    pytables:        3.4.4-py36_8                          conda-forge --> 3.4.4-py36ha205bf6_0 \n",
      "    scikit-learn:    0.19.1-py36_blas_openblas_201         conda-forge [blas_openblas] --> 0.19.1-py36h7aa7ec6_0\n",
      "    scipy:           1.1.0-py36_blas_openblas_200          conda-forge [blas_openblas] --> 1.1.0-py36hfc37229_0 \n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "libnetcdf 4.6.1: ####################################################### | 100% \n",
      "xarray 0.10.8: ######################################################### | 100% \n",
      "pytables 3.4.4: ######################################################## | 100% \n",
      "h5py 2.8.0: ############################################################ | 100% \n",
      "openssl 1.0.2o: ######################################################## | 100% \n",
      "netcdf4 1.4.0: ######################################################### | 100% \n",
      "hdf4 4.2.13: ########################################################### | 100% \n",
      "cftime 1.0.0b1: ######################################################## | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y xarray netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import boto3\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from math import trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that import level names from a txt file\n",
    "def get_levels():\n",
    "    filename = \"levels.txt\"\n",
    "    resource.Bucket(\"fsoi\").download_file(filename, filename)\n",
    "\n",
    "    f = open(filename, \"r\")\n",
    "    levels = f.read().split(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    os.remove(filename)\n",
    "\n",
    "    return levels\n",
    "\n",
    "\n",
    "# define a function that loads obs and bkg data into pandas dataframes\n",
    "def load_data(filepath, date):\n",
    "    resource = boto3.resource(\"s3\")\n",
    "    filename = \"/tmp/\" + filepath.split(\"/\")[-1]\n",
    "    resource.Bucket(\"fsoi\").download_file(filepath, filename)\n",
    "    \n",
    "    obs = pd.read_hdf(filename).xs([\"AIRS_AQUA\", 787], level=[\"PLATFORM\", \"CHANNEL\"])\n",
    "    os.remove(filename)\n",
    "    \n",
    "    obs = obs.reset_index(level=[0, 1])\n",
    "    obs = obs.drop([\"OBTYPE\", \"OBERR\", \"PRESSURE\"], axis=1)\n",
    "    \n",
    "    # fix lon between -180/180 instead of 0/360\n",
    "    mask_lon = obs[obs[\"LONGITUDE\"] > 180].index.tolist()\n",
    "    obs.loc[mask_lon, \"LONGITUDE\"] = obs.loc[mask_lon, \"LONGITUDE\"] - 360\n",
    "    \n",
    "    date = date[:-2] + \"_\" + date[-2:]\n",
    "    month = date[:4] + \"_\" + date[4:6] + \"/\"\n",
    "    filename = \"e5130_hyb_01.bkg.eta.\" + date + \"z.nc4\"\n",
    "    filepath = \"bkg/\" + month + filename\n",
    "    filename = \"/tmp/\" + filename\n",
    "    resource.Bucket(\"fsoi\").download_file(filepath, filename)\n",
    "    \n",
    "    bkg = xr.open_dataset(filename).to_dataframe()\\\n",
    "            .reset_index(level=[0, 1, 2])\n",
    "    \n",
    "    os.remove(filename)\n",
    "            \n",
    "    obs = obs.sort_values([\"LONGITUDE\", \"LATITUDE\"]).reset_index(drop=True)\n",
    "    bkg = bkg.sort_values([\"lon\", \"lat\"]).reset_index(drop=True)\n",
    "        \n",
    "    return obs, bkg\n",
    "\n",
    "\n",
    "# define kdtree function that return pts1 indexes of nearest point from pts2\n",
    "def do_kdtree(pts1, pts2):\n",
    "    mytree = cKDTree(pts1)\n",
    "    dist, indexes = mytree.query(pts2)\n",
    "    return indexes\n",
    "\n",
    "\n",
    "# define a function that gets nearest bkg point for each observation\n",
    "def get_nearest(obs, bkg):\n",
    "    obs_pts = obs[[\"LONGITUDE\", \"LATITUDE\"]].as_matrix()\n",
    "    bkg_pts = bkg[[\"lon\", \"lat\"]].as_matrix()\n",
    "    \n",
    "    nearest_ix = do_kdtree(bkg_pts, obs_pts)\n",
    "    bkg_nearest = bkg.iloc[nearest_ix]\n",
    "    \n",
    "    return bkg_nearest\n",
    "\n",
    "\n",
    "# define a function that gets bkg levels data for a particular point\n",
    "def get_lev_data(bkg, ix, lev_cols):\n",
    "    return bkg.loc[ix:ix + 71, lev_cols[1:]].copy().stack().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# define a function that adds level data from bkg 3D to 2D by transposing it\n",
    "def add_lev_data(bkg, bkg_2D, level_cols, levels):\n",
    "    lev_data = []\n",
    "    bkg_2D.index.map(lambda ix: lev_data.append(get_lev_data(bkg, ix, level_cols)))\n",
    "    lev_data = pd.concat(lev_data, axis=1).transpose()\n",
    "    \n",
    "    # set level column names (!! levels are different depending on the files)\n",
    "    # levels = [round(lev, 3) for lev in bkg.lev.unique()]\n",
    "    lev_data.columns = [\n",
    "            col + '_' + lev\n",
    "            for lev in levels\n",
    "            for col in level_cols[1:]\n",
    "            ]\n",
    "    \n",
    "    bkg_2D.reset_index(drop=True, inplace=True)\n",
    "    bkg_2D = pd.concat([bkg_2D, lev_data], axis=1)\n",
    "    \n",
    "    return bkg_2D\n",
    "\n",
    "\n",
    "# define a function that merges our training data from all files\n",
    "def merge_train_data(filepaths, levels, i, n):\n",
    "    level_cols = [\"lev\", \"delp\", \"u\", \"v\", \"tv\", \"sphu\", \"ozone\", \"qitot\", \"qltot\"]\n",
    "    ml_data = pd.DataFrame()\n",
    "    \n",
    "    for j in range(i, len(filepaths), n):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        filepath = filepaths[j]\n",
    "        date = re.findall(\"[0-9]+.h5\", filepath)[0][:-3]\n",
    "        obs, bkg = load_data(filepath, date)\n",
    "        \n",
    "        mask_2D = np.arange(0, len(bkg), 72)\n",
    "        bkg_2D = bkg.drop(level_cols, axis=1).loc[mask_2D, :]\n",
    "        \n",
    "        bkg_nearest = get_nearest(obs, bkg_2D)\n",
    "        bkg_nearest = add_lev_data(bkg, bkg_nearest, level_cols, levels)\n",
    "        \n",
    "        merge = pd.concat([obs, bkg_nearest], axis=1)\n",
    "        ml_data = pd.concat([ml_data, merge], axis=0)\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Merge {} obs and bkg done in: {} min and {} sec\".format(\n",
    "            date, trunc((end - start)/60), round((end - start)%60)\n",
    "        ))\n",
    "        \n",
    "    return ml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "787",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2525, in get_loc\n    return self._engine.get_loc(key)\n  File \"pandas/_libs/index.pyx\", line 117, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 139, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 811, in pandas._libs.hashtable.Int64HashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 817, in pandas._libs.hashtable.Int64HashTable.get_item\nKeyError: 787\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"<ipython-input-4-30fddecd0ddb>\", line 88, in merge_train_data\n    obs, bkg = load_data(filepath, date)\n  File \"<ipython-input-4-30fddecd0ddb>\", line 7, in load_data\n    obs = pd.read_hdf(filename).xs([\"AIRS_AQUA\", 787], level=[\"PLATFORM\", \"CHANNEL\"])\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/generic.py\", line 2323, in xs\n    drop_level=drop_level)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/multi.py\", line 2186, in get_loc_level\n    loc, new_index = self.get_loc_level(k, level=lev)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/multi.py\", line 2282, in get_loc_level\n    indexer = self._get_level_indexer(key, level=level)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/multi.py\", line 2362, in _get_level_indexer\n    loc = level_index.get_loc(key)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2527, in get_loc\n    return self._engine.get_loc(self._maybe_cast_indexer(key))\n  File \"pandas/_libs/index.pyx\", line 117, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 139, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 811, in pandas._libs.hashtable.Int64HashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 817, in pandas._libs.hashtable.Int64HashTable.get_item\nKeyError: 787\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0d1263bfe1e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         ]\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# merge our training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0d1263bfe1e1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m         ]\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# merge our training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 787"
     ]
    }
   ],
   "source": [
    "pstart = time.time()\n",
    "\n",
    "client = boto3.client(\"s3\")\n",
    "resource = boto3.resource(\"s3\")\n",
    "\n",
    "months = [\"2014_12\", \"2015_01\", \"2015_02\"]\n",
    "filepaths = []\n",
    "\n",
    "for month in months:\n",
    "    monthpaths = client.list_objects(Bucket=\"fsoi\", Prefix=\"obs/GMAO_\" + month + \"/GMAO.dry.\")\n",
    "    filepaths += [dic[\"Key\"] for dic in monthpaths[\"Contents\"]]\n",
    "\n",
    "levels = get_levels()\n",
    "\n",
    "os.remove(filename)\n",
    "\n",
    "# define how many process we want to run\n",
    "n = 12\n",
    "pool = mp.Pool(processes=n)\n",
    "\n",
    "# run our processes\n",
    "results = [\n",
    "        pool.apply_async(merge_train_data, args=(filepaths, levels, i, n))\n",
    "        for i in range(n)\n",
    "        ]\n",
    "\n",
    "results = [p.get() for p in results]\n",
    "\n",
    "# merge our training data\n",
    "ml_data = pd.concat(results, axis=0)\n",
    "\n",
    "# save and compress training data in hdf5 format\n",
    "start = time.time()\n",
    "\n",
    "filename = \"airs_aqua_ch787.h5\"\n",
    "key = \"ml_data/\" + filename\n",
    "filename = \"/tmp/\" + filename\n",
    "\n",
    "ml_data = ml_data.sort_values([\"DATETIME\", \"LATITUDE\", \"LONGITUDE\"])\\\n",
    "                 .reset_index(drop=True)\n",
    "                             \n",
    "ml_data.to_hdf(filename, key=\"df\", complevel=9)\n",
    "\n",
    "resource.Bucket(\"fsoi\").upload_file(filename, key)\n",
    "\n",
    "os.remove(filename)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Saved and compressed in: {} min and {} sec\".format(trunc((end - start)/60),\n",
    "                                                          round((end - start)%60)))\n",
    "    \n",
    "# display total program time\n",
    "pend = time.time()\n",
    "print(\"Total program took: {} hours and {} min\".format(trunc((pend - pstart)/3600),\n",
    "                                                       round((pend - pstart)%3600/60)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
